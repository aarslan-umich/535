{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aa2cb87f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    \\n    while True:\\n        byte = f.read(1)\\n        if not byte : \\n            break\\n        bits = bin(ord(byte))[2:]  # Python 3: byte[0] da kullanılabilir\\n        print(bits)\\n    '"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#images taken from https://unsplash.com/wallpapers\n",
    "from random import randrange\n",
    "images = [\"1.jpg\"]\n",
    "totalParity=0\n",
    "totalHamming=0\n",
    "dataSize= {\"parity\":7,\"hamming\":11}\n",
    "#dataSize= {\"parity\":7,\"hamming\":4}\n",
    "data = {\"packet\" : [] , \"ptype\" :[], \"label\" : []}\n",
    "\n",
    "def createParity(bitstr,parity=\"even\"):\n",
    "    \n",
    "    binaryCount = bitstr.count(\"1\")\n",
    "    if parity==\"even\":\n",
    "        if binaryCount %2 ==0 : \n",
    "            returnValue = bitstr+\"0\"\n",
    "        else :\n",
    "            returnValue = bitstr+\"1\"\n",
    "    else:\n",
    "        # odd parity\n",
    "        if binaryCount %2 ==1:\n",
    "            returnValue = bitstr+\"0\"\n",
    "        else : \n",
    "            returnValue = bitstr+\"1\"\n",
    "    return returnValue\n",
    "\n",
    "def createHamming(bitstr):\n",
    "    #iam going to use hamming 15 11 means 4 bits parity 1 2 4 8 bits parity bits total bits are \n",
    "    '''\n",
    "        p1 = 1 3 5 7 9 11 13 15\n",
    "        p2 = 2 3 6 7 10 11 14 15\n",
    "        p3 = 4 5 6 7 12 13 14 15\n",
    "        p4 = 8 9 10 11 12 13 14 15\n",
    "    '''\n",
    "    packet = list(bitstr)\n",
    "    packet.insert(0,\"p\")\n",
    "    packet.insert(1,\"p\")\n",
    "    packet.insert(3,\"p\")\n",
    "    packet.insert(7,\"p\")\n",
    "    p1=\"\"\n",
    "    p2=\"\"\n",
    "    p3=\"\"\n",
    "    p4=\"\"\n",
    "    \n",
    "    for i in range(1,16):\n",
    "    #for i in range(1,8):\n",
    "        if bin(i)[2:].zfill(4)[-1]==\"1\" and i!=1:\n",
    "            p1+=packet[i-1]\n",
    "        if bin(i)[2:].zfill(4)[-2]==\"1\" and i!=2:\n",
    "            p2+=packet[i-1]\n",
    "        if bin(i)[2:].zfill(4)[-3]==\"1\" and i!=4:\n",
    "            p3+=packet[i-1]\n",
    "        if bin(i)[2:].zfill(4)[-4]==\"1\" and i!=8:\n",
    "            p4+=packet[i-1]\n",
    "    packet[0] = \"0\" if p1.count(\"1\")%2 ==0 else \"1\"\n",
    "    packet[1] = \"0\" if p2.count(\"1\")%2 ==0 else \"1\"\n",
    "    packet[3] = \"0\" if p3.count(\"1\")%2 ==0 else \"1\"\n",
    "    packet[7] = \"0\" if p4.count(\"1\")%2 ==0 else \"1\"\n",
    "    return ''.join(packet)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for image in images : \n",
    "    dosyaStart=0\n",
    "    f=open(image,\"rb\")\n",
    "    file_bytes = f.read()\n",
    "    dosyaStop = len(file_bytes)*8 # converting bytes to bits\n",
    "    bits = ''.join(format(byte, '08b') for byte in file_bytes)\n",
    "    while dosyaStart < dosyaStop :\n",
    "        label=0\n",
    "        #myRandom = randrange(1,3)\n",
    "        myRandom=2\n",
    "        if myRandom ==1:\n",
    "            totalParity+=1\n",
    "            packet = bits[dosyaStart:dosyaStart+dataSize[\"parity\"]]\n",
    "            if len(packet)<dataSize[\"parity\"]:\n",
    "                break;\n",
    "            dosyaStart+=dataSize[\"parity\"]\n",
    "            #evenOrOdd = randrange(1,3)\n",
    "            evenOrOdd = 2\n",
    "            if evenOrOdd == 1:\n",
    "                ptype=\"odd\"\n",
    "                #packet = createParity(packet,\"odd\") sonuc lineer olmadigi icin sorun cikariyor\n",
    "                packet = createParity(packet,\"even\")\n",
    "            else : \n",
    "                ptype=\"even\"\n",
    "                packet = createParity(packet)\n",
    "            if totalParity%2==0:\n",
    "                label=1\n",
    "                corruptbit = randrange(0,7)\n",
    "                bit = packet[corruptbit]\n",
    "                if bit == \"1\":\n",
    "                    bit=\"0\"\n",
    "                else:\n",
    "                    bit=\"1\"\n",
    "                packet=packet[:corruptbit] + bit + packet[corruptbit+1:]\n",
    "\n",
    "        else : \n",
    "            totalHamming+=1\n",
    "            #hamming code\n",
    "            ptype=\"hamming\"\n",
    "            packet = bits[dosyaStart:dosyaStart+dataSize[\"hamming\"]]\n",
    "            dosyaStart+=dataSize[\"hamming\"]\n",
    "            if len(packet)<dataSize[\"hamming\"]:\n",
    "                break;\n",
    "            packet=createHamming(packet)\n",
    "            datapackets = [2,4,5,6,8,9,10,11,12,13,14]\n",
    "            #datapackets = [2,4,5,6]\n",
    "            if totalHamming%3==1:\n",
    "                label=1\n",
    "                corruptbit=datapackets[randrange(0,len(datapackets))]\n",
    "                bit=packet[corruptbit]\n",
    "                if bit==\"1\":\n",
    "                    bit=\"0\"\n",
    "                else:\n",
    "                    bit=\"1\"\n",
    "                packet=packet[:corruptbit] + bit + packet[corruptbit+1:]\n",
    "\n",
    "        data[\"packet\"].append(packet)\n",
    "        data[\"ptype\"].append(ptype)\n",
    "        data[\"label\"].append(label)\n",
    "    f.close()\n",
    "\n",
    "'''    \n",
    "    while True:\n",
    "        byte = f.read(1)\n",
    "        if not byte : \n",
    "            break\n",
    "        bits = bin(ord(byte))[2:]  # Python 3: byte[0] da kullanılabilir\n",
    "        print(bits)\n",
    "    '''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d6011abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "822741a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label distribution: 0    1424087\n",
      "1     711677\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>packet</th>\n",
       "      <th>ptype</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>110111010111011</td>\n",
       "      <td>hamming</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001000001100000</td>\n",
       "      <td>hamming</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>011000101001110</td>\n",
       "      <td>hamming</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>001001010100101</td>\n",
       "      <td>hamming</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100011011010001</td>\n",
       "      <td>hamming</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>001000111011101</td>\n",
       "      <td>hamming</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>111100001111000</td>\n",
       "      <td>hamming</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>101100100000010</td>\n",
       "      <td>hamming</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>100011111101101</td>\n",
       "      <td>hamming</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>001111011011101</td>\n",
       "      <td>hamming</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            packet    ptype  label\n",
       "0  110111010111011  hamming      0\n",
       "1  001000001100000  hamming      0\n",
       "2  011000101001110  hamming      0\n",
       "3  001001010100101  hamming      1\n",
       "4  100011011010001  hamming      1\n",
       "5  001000111011101  hamming      0\n",
       "6  111100001111000  hamming      0\n",
       "7  101100100000010  hamming      1\n",
       "8  100011111101101  hamming      0\n",
       "9  001111011011101  hamming      0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.sample(frac=1, random_state=34).reset_index(drop=True)\n",
    "train_size = int(0.8 * len(df))\n",
    "traindf = df[:train_size]\n",
    "testdf  = df[train_size:]\n",
    "X_train = traindf[\"packet\"].tolist()  # .tolist() ile bit string listesi\n",
    "Y_train = traindf[\"label\"].tolist()\n",
    "X_test  = testdf[\"packet\"].tolist()\n",
    "Y_test  = testdf[\"label\"].tolist()\n",
    "\n",
    "print(\"Label distribution:\", pd.Series(Y_train).value_counts())\n",
    "traindf.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7b98b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e05281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Class distribution: Counter({0: 1424087, 1: 711677}), Focal alpha for class 1: 0.6668\n",
      "Epoch 1/10 | Loss: 0.077171 | Acc: 0.501 | Prec: 0.334 | Rec: 0.500 | F1: 0.400 | LR: 0.000084\n",
      "✅ Best model saved!\n",
      "Epoch 2/10 | Loss: 0.002892 | Acc: 0.989 | Prec: 0.980 | Rec: 0.986 | F1: 0.983 | LR: 0.000228\n",
      "✅ Best model saved!\n",
      "Epoch 3/10 | Loss: 0.000336 | Acc: 1.000 | Prec: 0.999 | Rec: 1.000 | F1: 0.999 | LR: 0.000300\n",
      "✅ Best model saved!\n",
      "Epoch 4/10 | Loss: 0.004200 | Acc: 0.979 | Prec: 0.960 | Rec: 0.979 | F1: 0.969 | LR: 0.000285\n",
      "Epoch 5/10 | Loss: 0.000492 | Acc: 0.999 | Prec: 0.999 | Rec: 0.998 | F1: 0.999 | LR: 0.000244\n",
      "Epoch 6/10 | Loss: 0.000294 | Acc: 1.000 | Prec: 1.000 | Rec: 0.999 | F1: 0.999 | LR: 0.000183\n",
      "✅ Best model saved!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 275\u001b[0m\n\u001b[0;32m    273\u001b[0m logits \u001b[38;5;241m=\u001b[39m model(x_batch)\n\u001b[0;32m    274\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(logits, y_batch)\n\u001b[1;32m--> 275\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    276\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[0;32m    277\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# ---------------- Positional Encoding ----------------\n",
    "\n",
    "\n",
    "class PosEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=15):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(100.0) / d_model))  \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        power_of_2_mask = torch.ones(max_len)\n",
    "        for i in [0, 1, 3, 7]:  \n",
    "            if i < max_len:\n",
    "                power_of_2_mask[i] = 2.0  \n",
    "        pe = pe * power_of_2_mask.unsqueeze(1)  \n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1), :].to(x.device)\n",
    "\n",
    "\n",
    "# ---------------- Multi-Head Attention ----------------\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, seq_len=15, pad_idx=9):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.q = nn.Linear(d_model, d_model)\n",
    "        self.k = nn.Linear(d_model, d_model)\n",
    "        self.v = nn.Linear(d_model, d_model)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "        # Hamming(15,11) parity map\n",
    "        self.parity_map = {\n",
    "            0: [2,4,6,8,10,12,14],   # P1\n",
    "            1: [2,5,6,9,10,13,14],   # P2\n",
    "            3: [4,5,6,11,12,13,14],  # P3\n",
    "            7: [8,9,10,11,12,13,14], # P4\n",
    "        }\n",
    "        mask = torch.zeros(seq_len, seq_len)\n",
    "        for p, bits in self.parity_map.items():\n",
    "            mask[p, bits] = 1\n",
    "        for i in range(seq_len):\n",
    "            mask[i,i] = 1\n",
    "        self.register_buffer('hamming_mask', mask)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        b, seq_len, d_model = x.size()\n",
    "\n",
    "        Q = self.q(x).view(b, seq_len, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        K = self.k(x).view(b, seq_len, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        V = self.v(x).view(b, seq_len, self.num_heads, self.head_dim).transpose(1,2)\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2,-1)) / math.sqrt(self.head_dim)\n",
    "\n",
    "        # Padding mask\n",
    "        if mask is not None:\n",
    "            pad_mask = mask.unsqueeze(1).unsqueeze(2).float()  # [B,1,1,L]\n",
    "        else:\n",
    "            pad_mask = torch.ones(b,1,1,seq_len, device=x.device)\n",
    "\n",
    "        # Hamming mask\n",
    "        hamming_mask = self.hamming_mask[:seq_len, :seq_len].unsqueeze(0).unsqueeze(1)  # [1,1,L,L]\n",
    "\n",
    "        # Combine\n",
    "        combined_mask = pad_mask * hamming_mask\n",
    "        scores = scores.masked_fill(combined_mask == 0, float('-inf'))\n",
    "\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        out = torch.matmul(attn, V)\n",
    "        out = out.transpose(1,2).contiguous().view(b, seq_len, d_model)\n",
    "        return self.out(out)\n",
    "\n",
    "\n",
    "\n",
    "# ---------------- Feed Forward ----------------\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "    def forward(self, x):\n",
    "        return self.fc2(F.relu(self.fc1(x)))\n",
    "\n",
    "\n",
    "# ---------------- Encoder Block ----------------\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = Attention(d_model, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.ff = FeedForward(d_model, d_ff)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.norm1(x + self.dropout(self.attn(x, mask)))\n",
    "        x = self.norm2(x + self.dropout(self.ff(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "# ---------------- Bit Transformer ----------------\n",
    "class BitTransformerClassifier(nn.Module):\n",
    "    def __init__(self, d_model=128, num_heads=8, d_ff=512, num_layers=8,\n",
    "                 num_classes=2, max_len=15, pad_idx=9):\n",
    "        super().__init__()\n",
    "        self.pad_idx = pad_idx\n",
    "        vocab_size = pad_idx + 1\n",
    "        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)\n",
    "        self.pos_enc = PosEncoding(d_model, max_len)\n",
    "        self.layers = nn.ModuleList([EncoderBlock(d_model, num_heads, d_ff) for _ in range(num_layers)])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.classifier = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mask = (x != self.pad_idx)\n",
    "        x = self.embed(x)\n",
    "        x = self.pos_enc(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        x = self.norm(x)\n",
    "        mask_float = mask.unsqueeze(-1).float()\n",
    "        x = (x * mask_float).sum(dim=1) / mask_float.sum(dim=1).clamp(min=1e-9)\n",
    "        return self.classifier(x)\n",
    "\n",
    "\n",
    "# ---------------- Focal Loss ----------------\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        if alpha is not None:\n",
    "            self.alpha = torch.tensor(alpha, dtype=torch.float)\n",
    "        else:\n",
    "            self.alpha = None\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        device = inputs.device\n",
    "        probs = F.softmax(inputs, dim=1).clamp(min=1e-9)\n",
    "        pt = probs.gather(1, targets.unsqueeze(1)).squeeze(1)\n",
    "        log_pt = torch.log(pt)\n",
    "        if self.alpha is not None:\n",
    "            alpha_t = self.alpha.to(device)[targets]  # <-- self.alpha GPU'ya taşındı\n",
    "        else:\n",
    "            alpha_t = torch.ones_like(pt, device=device)\n",
    "        loss = -alpha_t * ((1 - pt) ** self.gamma) * log_pt\n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "# ---------------- Dataset Preparation ----------------\n",
    "def prepare_tensor(bit_list, pad_idx=9, max_len=15):\n",
    "    x = []\n",
    "    for s in bit_list:\n",
    "        bits = [int(b) for b in str(s).strip()]\n",
    "        if len(bits) < max_len:\n",
    "            bits += [pad_idx] * (max_len - len(bits))\n",
    "        else:\n",
    "            bits = bits[-max_len:]\n",
    "        x.append(bits)\n",
    "    return torch.tensor(x, dtype=torch.long)\n",
    "\n",
    "\n",
    "# ---------------- Training Setup ----------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "\n",
    "d_model = 64\n",
    "num_heads = 8\n",
    "d_ff = 256\n",
    "num_layers = 6\n",
    "epochs = 10\n",
    "pad_idx = 9\n",
    "max_len_input = 15\n",
    "batch_size = 128\n",
    "\n",
    "model = BitTransformerClassifier(d_model, num_heads, d_ff, num_layers,\n",
    "                                 num_classes=2, max_len=max_len_input, pad_idx=pad_idx)\n",
    "model.to(device)\n",
    "\n",
    "# Class distribution\n",
    "class_counts = Counter(Y_train)\n",
    "total = sum(class_counts.values())\n",
    "alpha_for_1 = 1 - (class_counts[1] / total)\n",
    "print(f\"Class distribution: {class_counts}, Focal alpha for class 1: {alpha_for_1:.4f}\")\n",
    "\n",
    "criterion = FocalLoss(alpha=[1-alpha_for_1, alpha_for_1], gamma=2.0)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "steps_per_epoch = max(1, len(X_train)//batch_size)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr=3e-4, epochs=epochs, steps_per_epoch=steps_per_epoch\n",
    ")\n",
    "\n",
    "X_train_tensor = prepare_tensor(X_train, pad_idx=pad_idx, max_len=max_len_input)\n",
    "Y_train_tensor = torch.tensor(Y_train, dtype=torch.long)\n",
    "\n",
    "best_loss = float('inf')\n",
    "\n",
    "# ---------------- Training Loop ----------------\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    for i in range(0, len(X_train_tensor), batch_size):\n",
    "        x_batch = X_train_tensor[i:i+batch_size].to(device)\n",
    "        y_batch = Y_train_tensor[i:i+batch_size].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x_batch)\n",
    "        loss = criterion(logits, y_batch)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        running_loss += loss.item() * x_batch.size(0)\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "    epoch_loss = running_loss / len(X_train_tensor)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    prec = precision_score(all_labels, all_preds, zero_division=0)\n",
    "    rec = recall_score(all_labels, all_preds, zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_preds, zero_division=0)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} | Loss: {epoch_loss:.6f} | Acc: {acc:.3f} | \"\n",
    "          f\"Prec: {prec:.3f} | Rec: {rec:.3f} | F1: {f1:.3f} | LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "    if epoch_loss < best_loss:\n",
    "        best_loss = epoch_loss\n",
    "        torch.save(model.state_dict(), \"best_hamming_model.pt\")\n",
    "        print(\"Best model saved!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "671d6a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ali\\AppData\\Local\\Temp\\ipykernel_23800\\1683030758.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"best_hamming_model.pt\", map_location=device))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BitTransformerClassifier(\n",
       "  (embed): Embedding(10, 64, padding_idx=9)\n",
       "  (pos_enc): PosEncoding()\n",
       "  (layers): ModuleList(\n",
       "    (0-5): 6 x EncoderBlock(\n",
       "      (attn): Attention(\n",
       "        (q): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (k): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (v): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (out): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
       "      )\n",
       "      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  (classifier): Linear(in_features=64, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_idx = 9\n",
    "max_len_input = 15\n",
    "\n",
    "model = BitTransformerClassifier(\n",
    "    d_model=64,\n",
    "    num_heads=8,\n",
    "    d_ff=256,\n",
    "    num_layers=6,\n",
    "    num_classes=2,\n",
    "    max_len=max_len_input,\n",
    "    pad_idx=pad_idx\n",
    ")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.load_state_dict(torch.load(\"best_hamming_model.pt\", map_location=device))\n",
    "model.to(device)\n",
    "model.eval() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548c24fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111111100111000\n",
      "1\n",
      "['111111100111000']\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0]], device='cuda:0')\n",
      "tensor([1], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#X_test  = testdf[\"packet\"].tolist()\n",
    "#Y_test  = testdf[\"label\"].tolist()\n",
    "'''X_test_tensor = prepare_tensor(X_test, pad_idx=pad_idx, max_len=max_len_input).to(device)\n",
    "Y_test_tensor = torch.tensor(Y_test, dtype=torch.long).to(device)\n",
    "with torch.no_grad():\n",
    "    logits = model(X_test_tensor)\n",
    "    preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "acc = (preds == Y_test_tensor).float().mean().item()\n",
    "print(f\"Accuracy: {acc:.3f}\")'''\n",
    "\n",
    "print(X_test[7])\n",
    "print(Y_test[7])\n",
    "print(X_test[7].split())\n",
    "X_test_tensor = prepare_tensor(X_test[7].split(),pad_idx=pad_idx , max_len = max_len_input).to(device) \n",
    "Y_test_tensor = torch.tensor([Y_test[7]],dtype=torch.long , device=device)\n",
    "print(X_test_tensor)\n",
    "print(Y_test_tensor)\n",
    "\n",
    "\n",
    "batchsize = 512\n",
    "mean = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(0 , len(X_test),batchsize):\n",
    "        \n",
    "        X_test_tensor = prepare_tensor(X_test[i:i+batchsize],pad_idx=pad_idx , max_len = max_len_input).to(device) \n",
    "        Y_test_tensor = torch.tensor(Y_test[i:i+batchsize],dtype=torch.long , device=device)\n",
    "        logit = model(X_test_tensor)\n",
    "        pred = torch.argmax(logit,dim=1)\n",
    "        acc = (pred==Y_test_tensor).float().mean().item()\n",
    "        mean.append(acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "47069d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999999999428527\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "lastmean = np.mean(mean)\n",
    "print (lastmean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b4739b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02187165",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
