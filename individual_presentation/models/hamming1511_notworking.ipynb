{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fb4fc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#images taken from https://unsplash.com/wallpapers\n",
    "from random import randrange\n",
    "images = [\"1.jpg\"]\n",
    "totalParity=0\n",
    "totalHamming=0\n",
    "dataSize= {\"parity\":7,\"hamming\":11}\n",
    "data = {\"packet\" : [] , \"ptype\" :[], \"label\" : []}\n",
    "\n",
    "def createParity(bitstr,parity=\"even\"):\n",
    "    \n",
    "    binaryCount = bitstr.count(\"1\")\n",
    "    if parity==\"even\":\n",
    "        if binaryCount %2 ==0 : \n",
    "            returnValue = bitstr+\"0\"\n",
    "        else :\n",
    "            returnValue = bitstr+\"1\"\n",
    "    else:\n",
    "        # odd parity\n",
    "        if binaryCount %2 ==1:\n",
    "            returnValue = bitstr+\"0\"\n",
    "        else : \n",
    "            returnValue = bitstr+\"1\"\n",
    "    return returnValue\n",
    "\n",
    "def createHamming(bitstr):\n",
    "    #iam going to use hamming 15 11 means 4 bits parity 1 2 4 8 bits parity bits total bits are \n",
    "    '''\n",
    "        p1 = 1 3 5 7 9 11 13 15\n",
    "        p2 = 2 3 6 7 10 11 14 15\n",
    "        p3 = 4 5 6 7 12 13 14 15\n",
    "        p4 = 8 9 10 11 12 13 14 15\n",
    "    '''\n",
    "    packet = list(bitstr)\n",
    "    packet.insert(0,\"p\")\n",
    "    packet.insert(1,\"p\")\n",
    "    packet.insert(3,\"p\")\n",
    "    packet.insert(7,\"p\")\n",
    "    p1=\"\"\n",
    "    p2=\"\"\n",
    "    p3=\"\"\n",
    "    p4=\"\"\n",
    "    #for i in range(1,8):\n",
    "    for i in range(1,16):\n",
    "        if bin(i)[2:].zfill(3)[-1]==\"1\" and i!=1:\n",
    "            p1+=packet[i-1]\n",
    "        if bin(i)[2:].zfill(3)[-2]==\"1\" and i!=2:\n",
    "            p2+=packet[i-1]\n",
    "        if bin(i)[2:].zfill(3)[-3]==\"1\" and i!=4:\n",
    "            p3+=packet[i-1]\n",
    "        if bin(i)[2:].zfill(4)[-4]==\"1\" and i!=8:\n",
    "            p4+=packet[i-1]\n",
    "    packet[0] = \"0\" if p1.count(\"1\")%2 ==0 else \"1\"\n",
    "    packet[1] = \"0\" if p2.count(\"1\")%2 ==0 else \"1\"\n",
    "    packet[3] = \"0\" if p3.count(\"1\")%2 ==0 else \"1\"\n",
    "    packet[7] = \"0\" if p4.count(\"1\")%2 ==0 else \"1\"\n",
    "    return ''.join(packet)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for image in images : \n",
    "    dosyaStart=0\n",
    "    f=open(image,\"rb\")\n",
    "    file_bytes = f.read()\n",
    "    dosyaStop = len(file_bytes)*8 # converting bytes to bits\n",
    "    bits = ''.join(format(byte, '08b') for byte in file_bytes)\n",
    "    while dosyaStart < dosyaStop :\n",
    "        label=0\n",
    "        #myRandom = randrange(1,3)\n",
    "        myRandom=2\n",
    "        if myRandom ==1:\n",
    "            totalParity+=1\n",
    "            packet = bits[dosyaStart:dosyaStart+dataSize[\"parity\"]]\n",
    "            if len(packet)<dataSize[\"parity\"]:\n",
    "                break;\n",
    "            dosyaStart+=dataSize[\"parity\"]\n",
    "            evenOrOdd = randrange(1,3)\n",
    "            evenOrOdd=2\n",
    "            if evenOrOdd == 1:\n",
    "                ptype=\"odd\"\n",
    "                #packet = createParity(packet,\"odd\") sonuc lineer olmadigi icin sorun cikariyor\n",
    "                packet = createParity(packet,\"even\")\n",
    "            else : \n",
    "                ptype=\"even\"\n",
    "                packet = createParity(packet)\n",
    "            if totalParity%2==0:\n",
    "                label=1\n",
    "                corruptbit = randrange(0,7)\n",
    "                bit = packet[corruptbit]\n",
    "                if bit == \"1\":\n",
    "                    bit=\"0\"\n",
    "                else:\n",
    "                    bit=\"1\"\n",
    "                packet=packet[:corruptbit] + bit + packet[corruptbit+1:]\n",
    "\n",
    "        else : \n",
    "            totalHamming+=1\n",
    "            #hamming code\n",
    "            ptype=\"hamming\"\n",
    "            packet = bits[dosyaStart:dosyaStart+dataSize[\"hamming\"]]\n",
    "            dosyaStart+=dataSize[\"hamming\"]\n",
    "            if len(packet)<dataSize[\"hamming\"]:\n",
    "                break;\n",
    "            packet=createHamming(packet)\n",
    "            datapackets = [2,4,5,6,8,9,10,11,12,13,14]\n",
    "            #datapackets = [2,4,5,6]\n",
    "            if totalHamming%2==1:\n",
    "                label=1\n",
    "                corruptbit=datapackets[randrange(0,len(datapackets))]\n",
    "                bit=packet[corruptbit]\n",
    "                if bit==\"1\":\n",
    "                    bit=\"0\"\n",
    "                else:\n",
    "                    bit=\"1\"\n",
    "                packet=packet[:corruptbit] + bit + packet[corruptbit+1:]\n",
    "\n",
    "        data[\"packet\"].append(packet)\n",
    "        data[\"ptype\"].append(ptype)\n",
    "        data[\"label\"].append(label)\n",
    "    f.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "846890c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label distribution: 1    400506\n",
      "0    400405\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>packet</th>\n",
       "      <th>ptype</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>110111010111011</td>\n",
       "      <td>hamming</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001000001100001</td>\n",
       "      <td>hamming</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>011000101001111</td>\n",
       "      <td>hamming</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>001001010100101</td>\n",
       "      <td>hamming</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100011011010001</td>\n",
       "      <td>hamming</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>001000111011101</td>\n",
       "      <td>hamming</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>111100001111000</td>\n",
       "      <td>hamming</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>101100100000111</td>\n",
       "      <td>hamming</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>100011111101101</td>\n",
       "      <td>hamming</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>001111011011101</td>\n",
       "      <td>hamming</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>101110000100000</td>\n",
       "      <td>hamming</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>101000110011111</td>\n",
       "      <td>hamming</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>000100110010000</td>\n",
       "      <td>hamming</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>101111101000010</td>\n",
       "      <td>hamming</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>000000111110011</td>\n",
       "      <td>hamming</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             packet    ptype  label\n",
       "0   110111010111011  hamming      0\n",
       "1   001000001100001  hamming      1\n",
       "2   011000101001111  hamming      1\n",
       "3   001001010100101  hamming      1\n",
       "4   100011011010001  hamming      1\n",
       "5   001000111011101  hamming      0\n",
       "6   111100001111000  hamming      0\n",
       "7   101100100000111  hamming      1\n",
       "8   100011111101101  hamming      0\n",
       "9   001111011011101  hamming      0\n",
       "10  101110000100000  hamming      1\n",
       "11  101000110011111  hamming      1\n",
       "12  000100110010000  hamming      0\n",
       "13  101111101000010  hamming      1\n",
       "14  000000111110011  hamming      1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame(data)\n",
    "df = df.sample(frac=1, random_state=34).reset_index(drop=True)\n",
    "train_size = int(0.3 * len(df))\n",
    "traindf = df[:train_size]\n",
    "testdf  = df[train_size:]\n",
    "X_train = traindf[\"packet\"].tolist()  # .tolist() ile bit string listesi\n",
    "Y_train = traindf[\"label\"].tolist()\n",
    "X_test  = testdf[\"packet\"].tolist()\n",
    "Y_test  = testdf[\"label\"].tolist()\n",
    "\n",
    "print(\"Label distribution:\", pd.Series(Y_train).value_counts())\n",
    "traindf.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e40603e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Class distribution: Counter({1: 400506, 0: 400405}), Focal alpha for class 1: 0.4999\n",
      "Epoch 1/50 | Loss: 0.087043 | Acc: 0.500 | Prec: 0.500 | Rec: 0.500 | F1: 0.500 | LR: 0.000025\n",
      "✅ Best model saved!\n",
      "Epoch 2/50 | Loss: 0.086752 | Acc: 0.500 | Prec: 0.500 | Rec: 0.494 | F1: 0.497 | LR: 0.000041\n",
      "✅ Best model saved!\n",
      "Epoch 3/50 | Loss: 0.086709 | Acc: 0.500 | Prec: 0.500 | Rec: 0.490 | F1: 0.495 | LR: 0.000066\n",
      "✅ Best model saved!\n",
      "Epoch 4/50 | Loss: 0.086693 | Acc: 0.501 | Prec: 0.501 | Rec: 0.489 | F1: 0.495 | LR: 0.000099\n",
      "✅ Best model saved!\n",
      "Epoch 5/50 | Loss: 0.086682 | Acc: 0.500 | Prec: 0.500 | Rec: 0.486 | F1: 0.493 | LR: 0.000140\n",
      "✅ Best model saved!\n",
      "Epoch 6/50 | Loss: 0.086674 | Acc: 0.500 | Prec: 0.500 | Rec: 0.489 | F1: 0.495 | LR: 0.000186\n",
      "✅ Best model saved!\n",
      "Epoch 7/50 | Loss: 0.086666 | Acc: 0.500 | Prec: 0.501 | Rec: 0.489 | F1: 0.494 | LR: 0.000235\n",
      "✅ Best model saved!\n",
      "Epoch 8/50 | Loss: 0.086662 | Acc: 0.500 | Prec: 0.501 | Rec: 0.490 | F1: 0.495 | LR: 0.000285\n",
      "✅ Best model saved!\n",
      "Epoch 9/50 | Loss: 0.086663 | Acc: 0.500 | Prec: 0.500 | Rec: 0.486 | F1: 0.493 | LR: 0.000334\n",
      "Epoch 10/50 | Loss: 0.086664 | Acc: 0.500 | Prec: 0.500 | Rec: 0.487 | F1: 0.494 | LR: 0.000380\n",
      "Epoch 11/50 | Loss: 0.086665 | Acc: 0.500 | Prec: 0.500 | Rec: 0.487 | F1: 0.494 | LR: 0.000421\n",
      "Epoch 12/50 | Loss: 0.086666 | Acc: 0.500 | Prec: 0.500 | Rec: 0.483 | F1: 0.491 | LR: 0.000454\n",
      "Epoch 13/50 | Loss: 0.086667 | Acc: 0.500 | Prec: 0.500 | Rec: 0.487 | F1: 0.493 | LR: 0.000479\n",
      "Epoch 14/50 | Loss: 0.086661 | Acc: 0.500 | Prec: 0.500 | Rec: 0.487 | F1: 0.493 | LR: 0.000495\n",
      "✅ Best model saved!\n",
      "Epoch 15/50 | Loss: 0.086658 | Acc: 0.500 | Prec: 0.500 | Rec: 0.480 | F1: 0.490 | LR: 0.000500\n",
      "✅ Best model saved!\n",
      "Epoch 16/50 | Loss: 0.086656 | Acc: 0.500 | Prec: 0.500 | Rec: 0.481 | F1: 0.491 | LR: 0.000499\n",
      "✅ Best model saved!\n",
      "Epoch 17/50 | Loss: 0.086654 | Acc: 0.500 | Prec: 0.500 | Rec: 0.476 | F1: 0.488 | LR: 0.000496\n",
      "✅ Best model saved!\n",
      "Epoch 18/50 | Loss: 0.086652 | Acc: 0.500 | Prec: 0.501 | Rec: 0.477 | F1: 0.489 | LR: 0.000491\n",
      "✅ Best model saved!\n",
      "Epoch 19/50 | Loss: 0.086650 | Acc: 0.501 | Prec: 0.501 | Rec: 0.473 | F1: 0.487 | LR: 0.000484\n",
      "✅ Best model saved!\n",
      "Epoch 20/50 | Loss: 0.086651 | Acc: 0.501 | Prec: 0.501 | Rec: 0.465 | F1: 0.482 | LR: 0.000475\n",
      "Epoch 21/50 | Loss: 0.086648 | Acc: 0.501 | Prec: 0.501 | Rec: 0.473 | F1: 0.486 | LR: 0.000465\n",
      "✅ Best model saved!\n",
      "Epoch 22/50 | Loss: 0.086647 | Acc: 0.501 | Prec: 0.501 | Rec: 0.477 | F1: 0.489 | LR: 0.000452\n",
      "✅ Best model saved!\n",
      "Epoch 23/50 | Loss: 0.086647 | Acc: 0.501 | Prec: 0.501 | Rec: 0.485 | F1: 0.492 | LR: 0.000438\n",
      "✅ Best model saved!\n",
      "Epoch 24/50 | Loss: 0.086646 | Acc: 0.501 | Prec: 0.501 | Rec: 0.483 | F1: 0.491 | LR: 0.000423\n",
      "✅ Best model saved!\n",
      "Epoch 25/50 | Loss: 0.086646 | Acc: 0.501 | Prec: 0.501 | Rec: 0.486 | F1: 0.493 | LR: 0.000406\n",
      "✅ Best model saved!\n",
      "Epoch 26/50 | Loss: 0.086645 | Acc: 0.501 | Prec: 0.501 | Rec: 0.492 | F1: 0.496 | LR: 0.000388\n",
      "✅ Best model saved!\n",
      "Epoch 27/50 | Loss: 0.086645 | Acc: 0.501 | Prec: 0.501 | Rec: 0.489 | F1: 0.495 | LR: 0.000368\n",
      "✅ Best model saved!\n",
      "Epoch 28/50 | Loss: 0.086645 | Acc: 0.501 | Prec: 0.501 | Rec: 0.496 | F1: 0.498 | LR: 0.000348\n",
      "✅ Best model saved!\n",
      "Epoch 29/50 | Loss: 0.086645 | Acc: 0.501 | Prec: 0.501 | Rec: 0.484 | F1: 0.492 | LR: 0.000327\n",
      "✅ Best model saved!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 187\u001b[0m\n\u001b[0;32m    185\u001b[0m logits \u001b[38;5;241m=\u001b[39m model(x_batch)\n\u001b[0;32m    186\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(logits, y_batch)\n\u001b[1;32m--> 187\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    188\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[0;32m    189\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# ---------------- Positional Encoding ----------------\n",
    "class PosEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=15):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1), :].to(x.device)\n",
    "\n",
    "\n",
    "# ---------------- Multi-Head Attention ----------------\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.q = nn.Linear(d_model, d_model)\n",
    "        self.k = nn.Linear(d_model, d_model)\n",
    "        self.v = nn.Linear(d_model, d_model)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        b, seq_len, d_model = x.size()\n",
    "        Q = self.q(x).view(b, seq_len, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        K = self.k(x).view(b, seq_len, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        V = self.v(x).view(b, seq_len, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        scores = torch.matmul(Q, K.transpose(-2,-1)) / math.sqrt(self.head_dim)\n",
    "        if mask is not None:\n",
    "            mask_exp = mask.unsqueeze(1).unsqueeze(2)\n",
    "            scores = scores.masked_fill(mask_exp == 0, float('-inf'))\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        out = torch.matmul(attn, V)\n",
    "        out = out.transpose(1,2).contiguous().view(b, seq_len, d_model)\n",
    "        return self.out(out)\n",
    "\n",
    "\n",
    "# ---------------- Feed Forward ----------------\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "    def forward(self, x):\n",
    "        return self.fc2(F.relu(self.fc1(x)))\n",
    "\n",
    "\n",
    "# ---------------- Encoder Block ----------------\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = Attention(d_model, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.ff = FeedForward(d_model, d_ff)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.norm1(x + self.dropout(self.attn(x, mask)))\n",
    "        x = self.norm2(x + self.dropout(self.ff(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "# ---------------- Bit Transformer ----------------\n",
    "class BitTransformerClassifier(nn.Module):\n",
    "    def __init__(self, d_model=128, num_heads=8, d_ff=512, num_layers=8,\n",
    "                 num_classes=2, max_len=15, pad_idx=9):\n",
    "        super().__init__()\n",
    "        self.pad_idx = pad_idx\n",
    "        vocab_size = 3\n",
    "        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)\n",
    "        self.pos_enc = PosEncoding(d_model, max_len)\n",
    "        self.layers = nn.ModuleList([EncoderBlock(d_model, num_heads, d_ff) for _ in range(num_layers)])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.classifier = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mask = (x != self.pad_idx)\n",
    "        x = self.embed(x)\n",
    "        x = self.pos_enc(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        x = self.norm(x)\n",
    "        mask_float = mask.unsqueeze(-1).float()\n",
    "        x = (x * mask_float).sum(dim=1) / mask_float.sum(dim=1).clamp(min=1e-9)\n",
    "        return self.classifier(x)\n",
    "\n",
    "\n",
    "# ---------------- Focal Loss ----------------\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        if alpha is not None:\n",
    "            self.alpha = torch.tensor(alpha, dtype=torch.float)\n",
    "        else:\n",
    "            self.alpha = None\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        device = inputs.device\n",
    "        probs = F.softmax(inputs, dim=1).clamp(min=1e-9)\n",
    "        pt = probs.gather(1, targets.unsqueeze(1)).squeeze(1)\n",
    "        log_pt = torch.log(pt)\n",
    "        if self.alpha is not None:\n",
    "            alpha_t = self.alpha.to(device)[targets]  # <-- self.alpha GPU'ya taşındı\n",
    "        else:\n",
    "            alpha_t = torch.ones_like(pt, device=device)\n",
    "        loss = -alpha_t * ((1 - pt) ** self.gamma) * log_pt\n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "# ---------------- Dataset Preparation ----------------\n",
    "def prepare_tensor(bit_list, pad_idx=9, max_len=15):\n",
    "    x = []\n",
    "    for s in bit_list:\n",
    "        bits = [int(b) for b in str(s).strip()]\n",
    "        if len(bits) < max_len:\n",
    "            bits += [pad_idx] * (max_len - len(bits))\n",
    "        else:\n",
    "            bits = bits[-max_len:]\n",
    "        x.append(bits)\n",
    "    return torch.tensor(x, dtype=torch.long)\n",
    "\n",
    "\n",
    "# ---------------- Training Setup ----------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Model parametreleri\n",
    "d_model = 64\n",
    "num_heads = 8\n",
    "d_ff = 256\n",
    "num_layers = 6\n",
    "epochs = 50\n",
    "pad_idx = 2\n",
    "max_len_input = 15\n",
    "batch_size = 256\n",
    "\n",
    "model = BitTransformerClassifier(d_model, num_heads, d_ff, num_layers,\n",
    "                                 num_classes=2, max_len=max_len_input, pad_idx=pad_idx)\n",
    "model.to(device)\n",
    "\n",
    "# Class distribution\n",
    "class_counts = Counter(Y_train)\n",
    "total = sum(class_counts.values())\n",
    "alpha_for_1 = 1 - (class_counts[1] / total)\n",
    "print(f\"Class distribution: {class_counts}, Focal alpha for class 1: {alpha_for_1:.4f}\")\n",
    "\n",
    "criterion = FocalLoss(alpha=[1-alpha_for_1, alpha_for_1], gamma=2.0)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.01)\n",
    "steps_per_epoch = max(1, len(X_train)//batch_size)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr=5e-4, epochs=epochs, steps_per_epoch=steps_per_epoch\n",
    ")\n",
    "\n",
    "X_train_tensor = prepare_tensor(X_train, pad_idx=pad_idx, max_len=max_len_input)\n",
    "Y_train_tensor = torch.tensor(Y_train, dtype=torch.long)\n",
    "\n",
    "best_loss = float('inf')\n",
    "\n",
    "# ---------------- Training Loop ----------------\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    for i in range(0, len(X_train_tensor), batch_size):\n",
    "        x_batch = X_train_tensor[i:i+batch_size].to(device)\n",
    "        y_batch = Y_train_tensor[i:i+batch_size].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x_batch)\n",
    "        loss = criterion(logits, y_batch)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        running_loss += loss.item() * x_batch.size(0)\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "    epoch_loss = running_loss / len(X_train_tensor)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    prec = precision_score(all_labels, all_preds, zero_division=0)\n",
    "    rec = recall_score(all_labels, all_preds, zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_preds, zero_division=0)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} | Loss: {epoch_loss:.6f} | Acc: {acc:.3f} | \"\n",
    "          f\"Prec: {prec:.3f} | Rec: {rec:.3f} | F1: {f1:.3f} | LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "    if epoch_loss < best_loss:\n",
    "        best_loss = epoch_loss\n",
    "        torch.save(model.state_dict(), \"best_hamming74_model.pt\")\n",
    "        print(\"Best model saved!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfb066e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
