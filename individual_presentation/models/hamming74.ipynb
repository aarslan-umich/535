{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fb4fc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#images taken from https://unsplash.com/wallpapers\n",
    "from random import randrange\n",
    "images = [\"1.jpg\"]\n",
    "totalParity=0\n",
    "totalHamming=0\n",
    "dataSize= {\"parity\":7,\"hamming\":4}\n",
    "data = {\"packet\" : [] , \"ptype\" :[], \"label\" : []}\n",
    "\n",
    "def createParity(bitstr,parity=\"even\"):\n",
    "    \n",
    "    binaryCount = bitstr.count(\"1\")\n",
    "    if parity==\"even\":\n",
    "        if binaryCount %2 ==0 : \n",
    "            returnValue = bitstr+\"0\"\n",
    "        else :\n",
    "            returnValue = bitstr+\"1\"\n",
    "    else:\n",
    "        # odd parity\n",
    "        if binaryCount %2 ==1:\n",
    "            returnValue = bitstr+\"0\"\n",
    "        else : \n",
    "            returnValue = bitstr+\"1\"\n",
    "    return returnValue\n",
    "\n",
    "def createHamming(bitstr):\n",
    "    #iam going to use hamming 15 11 means 4 bits parity 1 2 4 8 bits parity bits total bits are \n",
    "    '''\n",
    "        p1 = 1 3 5 7 9 11 13 15\n",
    "        p2 = 2 3 6 7 10 11 14 15\n",
    "        p3 = 4 5 6 7 12 13 14 15\n",
    "        p4 = 8 9 10 11 12 13 14 15\n",
    "    '''\n",
    "    packet = list(bitstr)\n",
    "    packet.insert(0,\"p\")\n",
    "    packet.insert(1,\"p\")\n",
    "    packet.insert(3,\"p\")\n",
    "    #packet.insert(7,\"p\")\n",
    "    p1=\"\"\n",
    "    p2=\"\"\n",
    "    p3=\"\"\n",
    "    #p4=\"\"\n",
    "    for i in range(1,8):\n",
    "    #for i in range(1,16):\n",
    "        if bin(i)[2:].zfill(3)[-1]==\"1\" and i!=1:\n",
    "            p1+=packet[i-1]\n",
    "        if bin(i)[2:].zfill(3)[-2]==\"1\" and i!=2:\n",
    "            p2+=packet[i-1]\n",
    "        if bin(i)[2:].zfill(3)[-3]==\"1\" and i!=4:\n",
    "            p3+=packet[i-1]\n",
    "        #if bin(i)[2:].zfill(4)[-4]==\"1\" and i!=8:\n",
    "        #    p4+=packet[i-1]\n",
    "    packet[0] = \"0\" if p1.count(\"1\")%2 ==0 else \"1\"\n",
    "    packet[1] = \"0\" if p2.count(\"1\")%2 ==0 else \"1\"\n",
    "    packet[3] = \"0\" if p3.count(\"1\")%2 ==0 else \"1\"\n",
    "    #packet[7] = \"0\" if p4.count(\"1\")%2 ==0 else \"1\"\n",
    "    return ''.join(packet)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for image in images : \n",
    "    dosyaStart=0\n",
    "    f=open(image,\"rb\")\n",
    "    file_bytes = f.read()\n",
    "    dosyaStop = len(file_bytes)*8 # converting bytes to bits\n",
    "    bits = ''.join(format(byte, '08b') for byte in file_bytes)\n",
    "    while dosyaStart < dosyaStop :\n",
    "        label=0\n",
    "        #myRandom = randrange(1,3)\n",
    "        myRandom=2\n",
    "        if myRandom ==1:\n",
    "            totalParity+=1\n",
    "            packet = bits[dosyaStart:dosyaStart+dataSize[\"parity\"]]\n",
    "            if len(packet)<dataSize[\"parity\"]:\n",
    "                break;\n",
    "            dosyaStart+=dataSize[\"parity\"]\n",
    "            evenOrOdd = randrange(1,3)\n",
    "            evenOrOdd=2\n",
    "            if evenOrOdd == 1:\n",
    "                ptype=\"odd\"\n",
    "                #packet = createParity(packet,\"odd\") sonuc lineer olmadigi icin sorun cikariyor\n",
    "                packet = createParity(packet,\"even\")\n",
    "            else : \n",
    "                ptype=\"even\"\n",
    "                packet = createParity(packet)\n",
    "            if totalParity%2==0:\n",
    "                label=1\n",
    "                corruptbit = randrange(0,7)\n",
    "                bit = packet[corruptbit]\n",
    "                if bit == \"1\":\n",
    "                    bit=\"0\"\n",
    "                else:\n",
    "                    bit=\"1\"\n",
    "                packet=packet[:corruptbit] + bit + packet[corruptbit+1:]\n",
    "\n",
    "        else : \n",
    "            totalHamming+=1\n",
    "            #hamming code\n",
    "            ptype=\"hamming\"\n",
    "            packet = bits[dosyaStart:dosyaStart+dataSize[\"hamming\"]]\n",
    "            dosyaStart+=dataSize[\"hamming\"]\n",
    "            if len(packet)<dataSize[\"hamming\"]:\n",
    "                break;\n",
    "            packet=createHamming(packet)\n",
    "            #datapackets = [2,4,5,6,8,9,10,11,12,13,14]\n",
    "            datapackets = [2,4,5,6]\n",
    "            if totalHamming%2==1:\n",
    "                label=1\n",
    "                corruptbit=datapackets[randrange(0,len(datapackets))]\n",
    "                bit=packet[corruptbit]\n",
    "                if bit==\"1\":\n",
    "                    bit=\"0\"\n",
    "                else:\n",
    "                    bit=\"1\"\n",
    "                packet=packet[:corruptbit] + bit + packet[corruptbit+1:]\n",
    "\n",
    "        data[\"packet\"].append(packet)\n",
    "        data[\"ptype\"].append(ptype)\n",
    "        data[\"label\"].append(label)\n",
    "    f.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "846890c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label distribution: 0    367459\n",
      "1    366710\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>packet</th>\n",
       "      <th>ptype</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1011010</td>\n",
       "      <td>hamming</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0110111</td>\n",
       "      <td>hamming</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1001100</td>\n",
       "      <td>hamming</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1110001</td>\n",
       "      <td>hamming</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0110001</td>\n",
       "      <td>hamming</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0001110</td>\n",
       "      <td>hamming</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0100101</td>\n",
       "      <td>hamming</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0000000</td>\n",
       "      <td>hamming</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0101010</td>\n",
       "      <td>hamming</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0100101</td>\n",
       "      <td>hamming</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1100111</td>\n",
       "      <td>hamming</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1001100</td>\n",
       "      <td>hamming</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1101001</td>\n",
       "      <td>hamming</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0011011</td>\n",
       "      <td>hamming</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0111101</td>\n",
       "      <td>hamming</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     packet    ptype  label\n",
       "0   1011010  hamming      0\n",
       "1   0110111  hamming      1\n",
       "2   1001100  hamming      0\n",
       "3   1110001  hamming      1\n",
       "4   0110001  hamming      1\n",
       "5   0001110  hamming      1\n",
       "6   0100101  hamming      0\n",
       "7   0000000  hamming      0\n",
       "8   0101010  hamming      0\n",
       "9   0100101  hamming      0\n",
       "10  1100111  hamming      1\n",
       "11  1001100  hamming      0\n",
       "12  1101001  hamming      0\n",
       "13  0011011  hamming      1\n",
       "14  0111101  hamming      1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame(data)\n",
    "df = df.sample(frac=1, random_state=34).reset_index(drop=True)\n",
    "train_size = int(0.1 * len(df))\n",
    "traindf = df[:train_size]\n",
    "testdf  = df[train_size:]\n",
    "X_train = traindf[\"packet\"].tolist()  # .tolist() ile bit string listesi\n",
    "Y_train = traindf[\"label\"].tolist()\n",
    "X_test  = testdf[\"packet\"].tolist()\n",
    "Y_test  = testdf[\"label\"].tolist()\n",
    "\n",
    "print(\"Label distribution:\", pd.Series(Y_train).value_counts())\n",
    "traindf.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e40603e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Class distribution: Counter({0: 367459, 1: 366710}), Focal alpha for class 1: 0.5005\n",
      "Epoch 1/50 | Loss: 0.067168 | Acc: 0.685 | Prec: 0.749 | Rec: 0.557 | F1: 0.639 | LR: 0.000025\n",
      "✅ Best model saved!\n",
      "Epoch 2/50 | Loss: 0.056765 | Acc: 0.742 | Prec: 0.910 | Rec: 0.538 | F1: 0.676 | LR: 0.000041\n",
      "✅ Best model saved!\n",
      "Epoch 3/50 | Loss: 0.027959 | Acc: 0.911 | Prec: 0.984 | Rec: 0.836 | F1: 0.904 | LR: 0.000066\n",
      "✅ Best model saved!\n",
      "Epoch 4/50 | Loss: 0.002814 | Acc: 0.992 | Prec: 0.999 | Rec: 0.986 | F1: 0.992 | LR: 0.000099\n",
      "✅ Best model saved!\n",
      "Epoch 5/50 | Loss: 0.000144 | Acc: 1.000 | Prec: 1.000 | Rec: 1.000 | F1: 1.000 | LR: 0.000140\n",
      "✅ Best model saved!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 188\u001b[0m\n\u001b[0;32m    186\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(logits, y_batch)\n\u001b[0;32m    187\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m--> 188\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip_grad_norm_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_norm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    189\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    190\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\utils\\clip_grad.py:30\u001b[0m, in \u001b[0;36m_no_grad.<locals>._no_grad_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_no_grad_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 30\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\utils\\clip_grad.py:86\u001b[0m, in \u001b[0;36mclip_grad_norm_\u001b[1;34m(parameters, max_norm, norm_type, error_if_nonfinite, foreach)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (device, _), ([device_grads], _) \u001b[38;5;129;01min\u001b[39;00m grouped_grads\u001b[38;5;241m.\u001b[39mitems():  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (foreach \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m _has_foreach_support(device_grads, device)) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m     84\u001b[0m         foreach \u001b[38;5;129;01mand\u001b[39;00m _device_has_foreach_support(device)\n\u001b[0;32m     85\u001b[0m     ):\n\u001b[1;32m---> 86\u001b[0m         norms\u001b[38;5;241m.\u001b[39mextend(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_foreach_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_grads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m foreach:\n\u001b[0;32m     88\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m     89\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforeach=True was passed, but can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt use the foreach API on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;241m.\u001b[39mtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tensors\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     90\u001b[0m         )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# ---------------- Positional Encoding ----------------\n",
    "class PosEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=15):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1), :].to(x.device)\n",
    "\n",
    "\n",
    "# ---------------- Multi-Head Attention ----------------\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.q = nn.Linear(d_model, d_model)\n",
    "        self.k = nn.Linear(d_model, d_model)\n",
    "        self.v = nn.Linear(d_model, d_model)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        b, seq_len, d_model = x.size()\n",
    "        Q = self.q(x).view(b, seq_len, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        K = self.k(x).view(b, seq_len, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        V = self.v(x).view(b, seq_len, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        scores = torch.matmul(Q, K.transpose(-2,-1)) / math.sqrt(self.head_dim)\n",
    "        if mask is not None:\n",
    "            mask_exp = mask.unsqueeze(1).unsqueeze(2)\n",
    "            scores = scores.masked_fill(mask_exp == 0, float('-inf'))\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        out = torch.matmul(attn, V)\n",
    "        out = out.transpose(1,2).contiguous().view(b, seq_len, d_model)\n",
    "        return self.out(out)\n",
    "\n",
    "\n",
    "# ---------------- Feed Forward ----------------\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "    def forward(self, x):\n",
    "        return self.fc2(F.relu(self.fc1(x)))\n",
    "\n",
    "\n",
    "# ---------------- Encoder Block ----------------\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = Attention(d_model, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.ff = FeedForward(d_model, d_ff)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.norm1(x + self.dropout(self.attn(x, mask)))\n",
    "        x = self.norm2(x + self.dropout(self.ff(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "# ---------------- Bit Transformer ----------------\n",
    "class BitTransformerClassifier(nn.Module):\n",
    "    def __init__(self, d_model=128, num_heads=8, d_ff=512, num_layers=8,\n",
    "                 num_classes=2, max_len=15, pad_idx=9):\n",
    "        super().__init__()\n",
    "        self.pad_idx = pad_idx\n",
    "        vocab_size = 3\n",
    "        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)\n",
    "        self.pos_enc = PosEncoding(d_model, max_len)\n",
    "        self.layers = nn.ModuleList([EncoderBlock(d_model, num_heads, d_ff) for _ in range(num_layers)])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.classifier = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mask = (x != self.pad_idx)\n",
    "        x = self.embed(x)\n",
    "        x = self.pos_enc(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        x = self.norm(x)\n",
    "        mask_float = mask.unsqueeze(-1).float()\n",
    "        x = (x * mask_float).sum(dim=1) / mask_float.sum(dim=1).clamp(min=1e-9)\n",
    "        return self.classifier(x)\n",
    "\n",
    "\n",
    "# ---------------- Focal Loss ----------------\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        if alpha is not None:\n",
    "            self.alpha = torch.tensor(alpha, dtype=torch.float)\n",
    "        else:\n",
    "            self.alpha = None\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        device = inputs.device\n",
    "        probs = F.softmax(inputs, dim=1).clamp(min=1e-9)\n",
    "        pt = probs.gather(1, targets.unsqueeze(1)).squeeze(1)\n",
    "        log_pt = torch.log(pt)\n",
    "        if self.alpha is not None:\n",
    "            alpha_t = self.alpha.to(device)[targets]  # <-- self.alpha GPU'ya taşındı\n",
    "        else:\n",
    "            alpha_t = torch.ones_like(pt, device=device)\n",
    "        loss = -alpha_t * ((1 - pt) ** self.gamma) * log_pt\n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "# ---------------- Dataset Preparation ----------------\n",
    "def prepare_tensor(bit_list, pad_idx=9, max_len=15):\n",
    "    x = []\n",
    "    for s in bit_list:\n",
    "        bits = [int(b) for b in str(s).strip()]\n",
    "        if len(bits) < max_len:\n",
    "            bits += [pad_idx] * (max_len - len(bits))\n",
    "        else:\n",
    "            bits = bits[-max_len:]\n",
    "        x.append(bits)\n",
    "    return torch.tensor(x, dtype=torch.long)\n",
    "\n",
    "\n",
    "# ---------------- Training Setup ----------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Model parametreleri\n",
    "d_model = 64\n",
    "num_heads = 4\n",
    "d_ff = 128\n",
    "num_layers = 4\n",
    "epochs = 50\n",
    "pad_idx = 2\n",
    "max_len_input = 15\n",
    "batch_size = 256\n",
    "\n",
    "model = BitTransformerClassifier(d_model, num_heads, d_ff, num_layers,\n",
    "                                 num_classes=2, max_len=max_len_input, pad_idx=pad_idx)\n",
    "model.to(device)\n",
    "\n",
    "# Class distribution\n",
    "class_counts = Counter(Y_train)\n",
    "total = sum(class_counts.values())\n",
    "alpha_for_1 = 1 - (class_counts[1] / total)\n",
    "print(f\"Class distribution: {class_counts}, Focal alpha for class 1: {alpha_for_1:.4f}\")\n",
    "\n",
    "criterion = FocalLoss(alpha=[1-alpha_for_1, alpha_for_1], gamma=2.0)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.01)\n",
    "steps_per_epoch = max(1, len(X_train)//batch_size)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr=5e-4, epochs=epochs, steps_per_epoch=steps_per_epoch\n",
    ")\n",
    "\n",
    "X_train_tensor = prepare_tensor(X_train, pad_idx=pad_idx, max_len=max_len_input)\n",
    "Y_train_tensor = torch.tensor(Y_train, dtype=torch.long)\n",
    "\n",
    "best_loss = float('inf')\n",
    "\n",
    "# ---------------- Training Loop ----------------\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    for i in range(0, len(X_train_tensor), batch_size):\n",
    "        x_batch = X_train_tensor[i:i+batch_size].to(device)\n",
    "        y_batch = Y_train_tensor[i:i+batch_size].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x_batch)\n",
    "        loss = criterion(logits, y_batch)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        running_loss += loss.item() * x_batch.size(0)\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "    epoch_loss = running_loss / len(X_train_tensor)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    prec = precision_score(all_labels, all_preds, zero_division=0)\n",
    "    rec = recall_score(all_labels, all_preds, zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_preds, zero_division=0)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} | Loss: {epoch_loss:.6f} | Acc: {acc:.3f} | \"\n",
    "          f\"Prec: {prec:.3f} | Rec: {rec:.3f} | F1: {f1:.3f} | LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "    if epoch_loss < best_loss:\n",
    "        best_loss = epoch_loss\n",
    "        torch.save(model.state_dict(), \"best_hamming74_model.pt\")\n",
    "        print(\"Best model saved!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfb066e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
